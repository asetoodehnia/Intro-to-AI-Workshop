{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup\n",
    "\n",
    "First we need to lay out the problem we're working through. This consists of defining the possible states and actions, and optionally the actions that are legal at each state. In order to perform value iteration we'll also need the transition probabilities and rewards when moving from state to state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states = [\"cold\", \"warm\", \"overheated\"]\n",
    "actions = [\"slow\", \"fast\"]\n",
    "Legal_Actions = {\"cold\": [\"slow\",\"fast\"],\n",
    "                 \"warm\": [\"slow\",\"fast\"],\n",
    "                 \"overheated\": []\n",
    "                }\n",
    "\n",
    "transitions = {(\"cold\", \"slow\", \"cold\"): 1,\n",
    "               (\"cold\", \"fast\", \"cold\"): .5,\n",
    "               (\"cold\", \"slow\", \"warm\"): 0,\n",
    "               (\"cold\", \"fast\", \"warm\"): .5,\n",
    "               (\"cold\", \"slow\", \"overheated\"): 0,\n",
    "               (\"cold\", \"fast\", \"overheated\"): 0,\n",
    "               \n",
    "               (\"warm\", \"slow\", \"cold\"): .5,\n",
    "               (\"warm\", \"fast\", \"cold\"): 0,\n",
    "               (\"warm\", \"slow\", \"warm\"): .5,\n",
    "               (\"warm\", \"fast\", \"warm\"): 0,\n",
    "               (\"warm\", \"slow\", \"overheated\"): 0,\n",
    "               (\"warm\", \"fast\", \"overheated\"): 1,\n",
    "              }\n",
    "\n",
    "rewards = {(\"cold\", \"slow\"): 1,\n",
    "           (\"cold\", \"fast\"): 2,\n",
    "           \n",
    "           (\"warm\", \"slow\"): 1,\n",
    "           (\"warm\", \"fast\"): -10,\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value, Q, and Policy iteration\n",
    "\n",
    "Implement a simple function for each type of iteration (hint: they should be very similar)\n",
    "\n",
    "For reference, here is a copy of the Bellman Equation\n",
    "$$\\Large\n",
    "Q^*(s,a) =\\sum_{s'} T(s,a,s')[R(s,a,s') + \\gamma V^* (s')]\n",
    "$$\n",
    "\n",
    "Feel free to define any helper functions you want or keep everything in the same block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(states, actions, Legal_Actions, T, R, gamma, max_iter=100):\n",
    "    v = {}\n",
    "    \n",
    "    # *** YOUR CODE HERE ***\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_iteration(states, actions, Legal_Actions, T, R, gamma, max_iter=100):\n",
    "    q={}\n",
    "    v={}\n",
    "    \n",
    "    # *** YOUR CODE HERE ***\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_iteration(states, actions, Legal_Actions, T, R, gamma, max_iter=10):\n",
    "    pi = {}\n",
    "    q = {}\n",
    "    v = {}\n",
    "   \n",
    "    # *** YOUR CODE HERE ***\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_values = value_iteration(states, actions, Legal_Actions, transitions, rewards, 0.9)\n",
    "q_values = q_iteration(states, actions, Legal_Actions, transitions, rewards, 0.9)\n",
    "learned_policy = policy_iteration(states, actions, Legal_Actions, transitions, rewards, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert learned_policy[\"cold\"] == \"fast\", \"The policy chose the wrong action when the engine is cold\"\n",
    "assert learned_policy[\"warm\"] == \"slow\", \"The policy chose the wrong action when the engine is warm\"\n",
    "assert state_values[\"cold\"] == max([q_values[(\"cold\",a)] for a in Legal_Actions[\"cold\"]]), \"Value iteration did not choose the maximizing action\"\n",
    "assert np.isclose(state_values[\"cold\"], 15.5), \"Value iteration converged to the wrong value\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
